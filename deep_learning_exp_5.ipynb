{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 14826483,
          "sourceType": "datasetVersion",
          "datasetId": 9482297
        }
      ],
      "dockerImageVersionId": 31259,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rai8896/Deep_Learning_Lab/blob/main/deep_learning_exp_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber torch numpy\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T08:58:44.339051Z",
          "iopub.execute_input": "2026-02-13T08:58:44.339439Z",
          "iopub.status.idle": "2026-02-13T08:58:51.842012Z",
          "shell.execute_reply.started": "2026-02-13T08:58:44.339408Z",
          "shell.execute_reply": "2026-02-13T08:58:51.840922Z"
        },
        "id": "dTCrDnyURPvq",
        "outputId": "4f5b5bf7-7394-45f9-9da5-652dd9a5d420"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting pdfplumber\n  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m940.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nCollecting pdfminer.six==20251230 (from pdfplumber)\n  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-5.4.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (46.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\nDownloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-5.4.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20251230 pdfplumber-0.11.9 pypdfium2-5.4.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T08:59:05.143661Z",
          "iopub.execute_input": "2026-02-13T08:59:05.144409Z",
          "iopub.status.idle": "2026-02-13T08:59:10.671162Z",
          "shell.execute_reply.started": "2026-02-13T08:59:05.144368Z",
          "shell.execute_reply": "2026-02-13T08:59:10.669920Z"
        },
        "id": "e20htHg-RPvs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/kaggle/input/poems100/poems-100.pdf\"   # Colab upload path\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text + \" \"\n",
        "\n",
        "print(\"Characters:\", len(text))\n",
        "print(text[:800])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T08:59:29.287257Z",
          "iopub.execute_input": "2026-02-13T08:59:29.287641Z",
          "iopub.status.idle": "2026-02-13T08:59:35.358610Z",
          "shell.execute_reply.started": "2026-02-13T08:59:29.287607Z",
          "shell.execute_reply": "2026-02-13T08:59:35.357425Z"
        },
        "id": "L6tB7LkVRPvs",
        "outputId": "e54c698d-0668-4d60-9ac2-2719bb2d0256"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Characters: 133468\ntext\nO my Luve's like a red, red rose\nThat’s newly sprung in June;\nO my Luve's like the melodie\nThat’s sweetly play'd in tune.\nAs fair art thou, my bonnie lass,\nSo deep in luve am I:\nAnd I will luve thee still, my dear,\nTill a’ the seas gang dry:\nTill a’ the seas gang dry, my dear,\nAnd the rocks melt wi’ the sun:\nI will luve thee still, my dear,\nWhile the sands o’ life shall run.\nAnd fare thee well, my only Luve\nAnd fare thee well, a while!\nAnd I will come again, my Luve,\nTho’ it were ten thousand mile.\nThe rose is red,\nThe violet's blue,\nSugar is sweet,\nAnd so are you.\nHow do I love thee? Let me count the ways.\nI love thee to the depth and breadth and height\nMy soul can reach, when feeling out of sight\nFor the ends of being and ideal grace.\nI love thee to the level of every day's\nMost qui\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Cleaning"
      ],
      "metadata": {
        "id": "1rUSUwOMRPvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()\n",
        "text = re.sub(r'\\n+', ' ', text)\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "print(text[:500])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:00:14.803494Z",
          "iopub.execute_input": "2026-02-13T09:00:14.803901Z",
          "iopub.status.idle": "2026-02-13T09:00:14.833382Z",
          "shell.execute_reply.started": "2026-02-13T09:00:14.803870Z",
          "shell.execute_reply": "2026-02-13T09:00:14.831925Z"
        },
        "id": "AflPCSSBRPvu",
        "outputId": "28486b40-08e7-43ad-8dc8-2172c854ad12"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "text o my luves like a red red rose thats newly sprung in june o my luves like the melodie thats sweetly playd in tune as fair art thou my bonnie lass so deep in luve am i and i will luve thee still my dear till a the seas gang dry till a the seas gang dry my dear and the rocks melt wi the sun i will luve thee still my dear while the sands o life shall run and fare thee well my only luve and fare thee well a while and i will come again my luve tho it were ten thousand mile the rose is red the vi\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization & Vocabulary"
      ],
      "metadata": {
        "id": "l3dLNcXSRPvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "\n",
        "vocab = sorted(list(set(words)))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {w: i for i, w in enumerate(vocab)}\n",
        "ix_to_word = {i: w for w, i in word_to_ix.items()}\n",
        "\n",
        "print(\"Total words:\", len(words))\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:01:14.145815Z",
          "iopub.execute_input": "2026-02-13T09:01:14.146170Z",
          "iopub.status.idle": "2026-02-13T09:01:14.161289Z",
          "shell.execute_reply.started": "2026-02-13T09:01:14.146138Z",
          "shell.execute_reply": "2026-02-13T09:01:14.160103Z"
        },
        "id": "8hAkWxL2RPvv",
        "outputId": "278afc0a-a431-49c6-ca89-27cd5743fb38"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total words: 24677\nVocabulary size: 5440\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PART 1: RNN FROM SCRATCH (NUMPY)"
      ],
      "metadata": {
        "id": "wsmB49YsRPvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = np.zeros((self.Whh.shape[0], 1))\n",
        "        outputs = []\n",
        "        for x in inputs:\n",
        "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "            y = self.Why @ h + self.by\n",
        "            outputs.append(y)\n",
        "        return outputs\n",
        "\n",
        "print(\"NumPy RNN implemented ✔\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:01:38.270844Z",
          "iopub.execute_input": "2026-02-13T09:01:38.271211Z",
          "iopub.status.idle": "2026-02-13T09:01:38.279915Z",
          "shell.execute_reply.started": "2026-02-13T09:01:38.271182Z",
          "shell.execute_reply": "2026-02-13T09:01:38.278356Z"
        },
        "id": "XU7V_54aRPvw",
        "outputId": "f2969233-218d-474e-cb4a-63ad43188849"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "NumPy RNN implemented ✔\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 2: ONE-HOT ENCODING + PyTorch RNN"
      ],
      "metadata": {
        "id": "6Uq2ZpCTRPvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 5\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(len(words) - SEQ_LEN):\n",
        "    X.append([word_to_ix[w] for w in words[i:i+SEQ_LEN]])\n",
        "    Y.append(word_to_ix[words[i+SEQ_LEN]])\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "\n",
        "print(X.shape, Y.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:02:01.165645Z",
          "iopub.execute_input": "2026-02-13T09:02:01.166256Z",
          "iopub.status.idle": "2026-02-13T09:02:01.264831Z",
          "shell.execute_reply.started": "2026-02-13T09:02:01.166203Z",
          "shell.execute_reply": "2026-02-13T09:02:01.263657Z"
        },
        "id": "T5Zj-etDRPvx",
        "outputId": "d24eecd8-839d-4ace-9f8f-7aebdce12ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([24672, 5]) torch.Size([24672])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding Function"
      ],
      "metadata": {
        "id": "-Ml7tSakRPvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(x, vocab_size):\n",
        "    return torch.eye(vocab_size)[x]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:02:25.856237Z",
          "iopub.execute_input": "2026-02-13T09:02:25.856648Z",
          "iopub.status.idle": "2026-02-13T09:02:25.861869Z",
          "shell.execute_reply.started": "2026-02-13T09:02:25.856609Z",
          "shell.execute_reply": "2026-02-13T09:02:25.860902Z"
        },
        "id": "lPywJu9ORPvx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN Model (One-Hot Input)"
      ],
      "metadata": {
        "id": "d5f0IZH_RPvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_OneHot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model_oh = RNN_OneHot(vocab_size, 128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_oh.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:03:04.821271Z",
          "iopub.execute_input": "2026-02-13T09:03:04.821639Z",
          "iopub.status.idle": "2026-02-13T09:03:09.329547Z",
          "shell.execute_reply.started": "2026-02-13T09:03:04.821601Z",
          "shell.execute_reply": "2026-02-13T09:03:09.328449Z"
        },
        "id": "Vkl9nsXzRPvx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train One-Hot Model"
      ],
      "metadata": {
        "id": "CF4nMXyYRPvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    x_oh = one_hot_encode(X, vocab_size)\n",
        "    outputs = model_oh(x_oh)\n",
        "    loss = criterion(outputs, Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:03:24.865483Z",
          "iopub.execute_input": "2026-02-13T09:03:24.866018Z",
          "iopub.status.idle": "2026-02-13T09:04:05.478331Z",
          "shell.execute_reply.started": "2026-02-13T09:03:24.865981Z",
          "shell.execute_reply": "2026-02-13T09:04:05.477249Z"
        },
        "id": "gnDjimyFRPvy",
        "outputId": "d4edf100-088c-4943-82fd-e53f9be3d5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1, Loss: 8.6103\nEpoch 2, Loss: 8.5853\nEpoch 3, Loss: 8.5596\nEpoch 4, Loss: 8.5309\nEpoch 5, Loss: 8.4966\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 3: TRAINABLE WORD EMBEDDINGS**"
      ],
      "metadata": {
        "id": "EqJD11VBRPvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model_emb = RNN_Embedding(vocab_size, 50, 128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_emb.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:04:54.325459Z",
          "iopub.execute_input": "2026-02-13T09:04:54.326035Z",
          "iopub.status.idle": "2026-02-13T09:04:54.342349Z",
          "shell.execute_reply.started": "2026-02-13T09:04:54.325997Z",
          "shell.execute_reply": "2026-02-13T09:04:54.341012Z"
        },
        "id": "aAzDbLkkRPvy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Embedding Model"
      ],
      "metadata": {
        "id": "35Ae01TQRPvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_emb(X)\n",
        "    loss = criterion(outputs, Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:04:58.315638Z",
          "iopub.execute_input": "2026-02-13T09:04:58.316645Z",
          "iopub.status.idle": "2026-02-13T09:05:10.121054Z",
          "shell.execute_reply.started": "2026-02-13T09:04:58.316606Z",
          "shell.execute_reply": "2026-02-13T09:05:10.120138Z"
        },
        "id": "2NysTnkeRPvz",
        "outputId": "4983f016-6019-4a7f-91f6-00f786148638"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1, Loss: 8.6165\nEpoch 2, Loss: 8.5827\nEpoch 3, Loss: 8.5482\nEpoch 4, Loss: 8.5118\nEpoch 5, Loss: 8.4724\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Text"
      ],
      "metadata": {
        "id": "UpmE6VYZRPvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_words, n_words=20):\n",
        "    model.eval()\n",
        "    words_gen = start_words.copy()\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        x = torch.tensor([[word_to_ix[w] for w in words_gen[-SEQ_LEN:]]])\n",
        "        out = model(x)\n",
        "        next_word = ix_to_word[out.argmax().item()]\n",
        "        words_gen.append(next_word)\n",
        "\n",
        "    return \" \".join(words_gen)\n",
        "\n",
        "print(generate_text(model_emb, words[:5]))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-13T09:05:13.930413Z",
          "iopub.execute_input": "2026-02-13T09:05:13.930925Z",
          "iopub.status.idle": "2026-02-13T09:05:13.960452Z",
          "shell.execute_reply.started": "2026-02-13T09:05:13.930887Z",
          "shell.execute_reply": "2026-02-13T09:05:13.959685Z"
        },
        "id": "idNJVFQ4RPvz",
        "outputId": "190a3402-9d50-4d47-a687-72a1415bc466"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "text o my luves like brute of the sin of the shore only the shore only the shore only the shore only the shore only\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}